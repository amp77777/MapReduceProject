{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amp77777/MapReduceProject/blob/main/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzDOnVOY9W3w",
        "outputId": "6119aa5b-b2a7-4794-82fa-dbe4850f116a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (5.9.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install psutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck2bpBD-96t_",
        "outputId": "aa95226a-74cb-40d0-99f2-48c0a8e4f241"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running MapReduce-style experiments...\n",
            "\n",
            "\n",
            "=== Sorting Experiment | Input Size: 32 ===\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# MapReduce Systems Project\n",
        "# Parallel Sorting and Max-Value Aggregation\n",
        "# ==========================\n",
        "\n",
        "import time, os, random, heapq, psutil\n",
        "import threading\n",
        "import multiprocessing as mp\n",
        "\n",
        "# ---------------------------\n",
        "# Utility helpers\n",
        "# ---------------------------\n",
        "\n",
        "def current_rss():\n",
        "    \"\"\"Return current process RSS memory in bytes.\"\"\"\n",
        "    p = psutil.Process(os.getpid())\n",
        "    return p.memory_info().rss\n",
        "\n",
        "def human_bytes(n):\n",
        "    for unit in ['B','KB','MB','GB']:\n",
        "        if n < 1024.0:\n",
        "            return f\"{n:.2f}{unit}\"\n",
        "        n /= 1024.0\n",
        "    return f\"{n:.2f}TB\"\n",
        "\n",
        "# ---------------------------\n",
        "# Helper: Split array indices evenly among workers\n",
        "# ---------------------------\n",
        "def chunk_indices(n_total, n_chunks):\n",
        "    base = n_total // n_chunks\n",
        "    remainder = n_total % n_chunks\n",
        "    pairs, start = [], 0\n",
        "    for i in range(n_chunks):\n",
        "        extra = 1 if i < remainder else 0\n",
        "        end = start + base + extra\n",
        "        pairs.append((start, end))\n",
        "        start = end\n",
        "    return pairs\n",
        "\n",
        "# ==========================\n",
        "# PART 1: Parallel Sorting (MapReduce Style)\n",
        "# ==========================\n",
        "\n",
        "def threaded_sort_map(data, n_workers):\n",
        "    n = len(data)\n",
        "    pairs = chunk_indices(n, n_workers)\n",
        "    sorted_chunks = [None]*n_workers\n",
        "\n",
        "    def worker(i, s, e):\n",
        "        arr = data[s:e]\n",
        "        arr.sort()\n",
        "        sorted_chunks[i] = (i, arr)\n",
        "\n",
        "    threads = []\n",
        "    t0 = time.perf_counter()\n",
        "    rss_before = current_rss()\n",
        "\n",
        "    for i,(s,e) in enumerate(pairs):\n",
        "        t = threading.Thread(target=worker, args=(i,s,e))\n",
        "        t.start()\n",
        "        threads.append(t)\n",
        "    for t in threads:\n",
        "        t.join()\n",
        "\n",
        "    t1 = time.perf_counter()\n",
        "    rss_after = current_rss()\n",
        "\n",
        "    sorted_chunks.sort(key=lambda x:x[0])\n",
        "    merged = list(heapq.merge(*[c[1] for c in sorted_chunks]))\n",
        "    return merged, t1 - t0, rss_before, rss_after\n",
        "\n",
        "def process_sort_worker(args):\n",
        "    i, arr = args\n",
        "    arr.sort()\n",
        "    return (i, arr)\n",
        "\n",
        "def multiprocessing_sort_map(data, n_workers):\n",
        "    n = len(data)\n",
        "    pairs = chunk_indices(n, n_workers)\n",
        "    args = [(i, data[s:e]) for i,(s,e) in enumerate(pairs)]\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    rss_before = current_rss()\n",
        "    with mp.Pool(n_workers) as pool:\n",
        "        results = pool.map(process_sort_worker, args)\n",
        "    t1 = time.perf_counter()\n",
        "    rss_after = current_rss()\n",
        "\n",
        "    results.sort(key=lambda x:x[0])\n",
        "    merged = list(heapq.merge(*[r[1] for r in results]))\n",
        "    return merged, t1 - t0, rss_before, rss_after\n",
        "\n",
        "# ==========================\n",
        "# PART 2: Max-Value Aggregation with Constrained Shared Memory\n",
        "# ==========================\n",
        "\n",
        "def threaded_max_map_reduce(data, n_workers):\n",
        "    pairs = chunk_indices(len(data), n_workers)\n",
        "    shared = {'value': float('-inf')}\n",
        "    lock = threading.Lock()\n",
        "\n",
        "    def worker(s,e):\n",
        "        local_max = max(data[s:e]) if e > s else float('-inf')\n",
        "        with lock:\n",
        "            if local_max > shared['value']:\n",
        "                shared['value'] = local_max\n",
        "\n",
        "    threads = []\n",
        "    t0 = time.perf_counter()\n",
        "    rss_before = current_rss()\n",
        "\n",
        "    for (s,e) in pairs:\n",
        "        t = threading.Thread(target=worker, args=(s,e))\n",
        "        t.start()\n",
        "        threads.append(t)\n",
        "    for t in threads:\n",
        "        t.join()\n",
        "\n",
        "    t1 = time.perf_counter()\n",
        "    rss_after = current_rss()\n",
        "    return shared['value'], t1 - t0, rss_before, rss_after\n",
        "\n",
        "def multiprocessing_max_map_reduce(data, n_workers):\n",
        "    pairs = chunk_indices(len(data), n_workers)\n",
        "    shared_val = mp.Value('i', -2**31)\n",
        "    lock = mp.Lock()\n",
        "\n",
        "    def worker(local):\n",
        "        local_max = max(local) if local else -2**31\n",
        "        with lock:\n",
        "            if local_max > shared_val.value:\n",
        "                shared_val.value = int(local_max)\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    rss_before = current_rss()\n",
        "    procs = []\n",
        "    for (s,e) in pairs:\n",
        "        local = data[s:e]\n",
        "        p = mp.Process(target=worker, args=(local,))\n",
        "        p.start()\n",
        "        procs.append(p)\n",
        "    for p in procs:\n",
        "        p.join()\n",
        "    t1 = time.perf_counter()\n",
        "    rss_after = current_rss()\n",
        "    return shared_val.value, t1 - t0, rss_before, rss_after\n",
        "\n",
        "# ==========================\n",
        "# EXPERIMENTS\n",
        "# ==========================\n",
        "\n",
        "def run_sort_experiment(sizes=(32,131072), workers=(1,2,4,8)):\n",
        "    random.seed(0)\n",
        "    for n in sizes:\n",
        "        print(f\"\\n=== Sorting Experiment | Input Size: {n} ===\")\n",
        "        data = [random.randint(0,100000) for _ in range(n)]\n",
        "        for w in workers:\n",
        "            sorted_thread, t_thread, b1,a1 = threaded_sort_map(data, w)\n",
        "            sorted_proc, t_proc, b2,a2 = multiprocessing_sort_map(data, w)\n",
        "            if n == 32:\n",
        "                correct = sorted_thread == sorted(data) and sorted_proc == sorted(data)\n",
        "                print(f\"\\nWorkers={w} | Correctness: {'PASS' if correct else 'FAIL'}\")\n",
        "            print(f\"Threads  | Time={t_thread:.4f}s | RSS Δ={human_bytes(a1-b1)}\")\n",
        "            print(f\"Processes| Time={t_proc:.4f}s | RSS Δ={human_bytes(a2-b2)}\")\n",
        "\n",
        "def run_max_experiment(sizes=(32,131072), workers=(1,2,4,8)):\n",
        "    random.seed(1)\n",
        "    for n in sizes:\n",
        "        print(f\"\\n=== Max Aggregation | Input Size: {n} ===\")\n",
        "        data = [random.randint(-1000000,1000000) for _ in range(n)]\n",
        "        truth = max(data)\n",
        "        for w in workers:\n",
        "            val_thread, t_thread, b1,a1 = threaded_max_map_reduce(data, w)\n",
        "            val_proc, t_proc, b2,a2 = multiprocessing_max_map_reduce(data, w)\n",
        "            print(f\"\\nWorkers={w}:\")\n",
        "            print(f\"Threads  | Value={val_thread}, OK={val_thread==truth}, Time={t_thread:.6f}s\")\n",
        "            print(f\"Processes| Value={val_proc}, OK={val_proc==truth}, Time={t_proc:.6f}s\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mp.set_start_method('spawn', force=True)\n",
        "    print(\"Running MapReduce-style experiments...\\n\")\n",
        "    run_sort_experiment()\n",
        "    run_max_experiment()\n",
        "    print(\"\\nAll experiments completed.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtlyQlLtYB3lnWYglHRZS6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}